{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true,
      "authorship_tag": "ABX9TyPo9k48sl+qRGUp9n2r4tLq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CalonWibu/kingkong-ai/blob/main/qwin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sel 1: Instalasi Pustaka\n",
        "# PASTIKAN SEMUA BERJALAN di satu tempat\n",
        "!pip install -q transformers accelerate peft trl datasets bitsandbytes\n",
        "\n",
        "# Lanjutkan dengan mount Drive Anda\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3Jc_9ckKjZ1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sel 1: Instalasi Pustaka\n",
        "# 1. Instalasi library AI lainnya\n",
        "!pip install transformers accelerate peft trl datasets\n",
        "\n",
        "# 2. Instalasi bitsandbytes secara terpisah (kritis)\n",
        "# Kita akan menggunakan -U untuk memastikan upgrade ke versi terbaru\n",
        "!pip install -U bitsandbytes\n",
        "\n",
        "# 3. Import bitsandbytes secara eksplisit\n",
        "# Ini kadang membantu runtime mengenali pustaka yang baru diinstal\n",
        "import bitsandbytes\n",
        "print(f\"bitsandbytes version: {bitsandbytes.__version__}\")\n",
        "\n",
        "# 4. Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "VM-sZdGAjyUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPaSXxG7FAgv"
      },
      "outputs": [],
      "source": [
        "# Hapus tanda # di depan perintah untuk menjalankan instalasi dan mount drive\n",
        "# Jalankan sel ini pertama kali\n",
        "\n",
        "# 1. Instalasi Pustaka Wajib\n",
        "!pip install -q transformers accelerate peft bitsandbytes trl datasets\n",
        "\n",
        "# 2. Mount Google Drive untuk penyimpanan hasil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer\n",
        "import warnings\n",
        "\n",
        "# --- 0. Setup Awal ---\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# Peringatan: Pastikan Anda memilih GPU T4/A100 di Runtime -> Change runtime type!\n",
        "\n",
        "# --- 1. Konfigurasi Dasar ---\n",
        "model_id = \"Qwen/Qwen1.5-1.8B-Chat\"\n",
        "dataset_name = \"izzulgod/indonesian-conversation\"\n",
        "# OUTPUT DIRECTORY KRITIS: Simpan di Google Drive agar tidak hilang\n",
        "output_dir = \"/content/drive/MyDrive/Kingkong_AI_Adapter\"\n",
        "\n",
        "# --- 2. Konfigurasi Quantization (QLoRA Setup untuk Colab GPU) ---\n",
        "print(\"üöÄ Mengatur konfigurasi 4-bit (QLoRA)...\")\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16, # Menggunakan bfloat16 (Optimal untuk Colab T4/A100)\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# --- 3. Muat Model dan Tokenizer ---\n",
        "print(\"üß† Memuat Model dan Tokenizer...\")\n",
        "\n",
        "try:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    model.config.use_cache = False\n",
        "    model.config.pretraining_tp = 1\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    print(\"‚úÖ Model dan Tokenizer berhasil dimuat.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Gagal memuat model: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 4. Konfigurasi LoRA ---\n",
        "print(\"‚öôÔ∏è Mengatur konfigurasi LoRA...\")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=64,\n",
        "    target_modules=\"all-linear\",\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# --- 5. Muat dan Format Dataset (Termasuk Kingkong AI dan LALY System Prompt) ---\n",
        "print(\"üìö Memuat dan memformat dataset...\")\n",
        "\n",
        "dataset = load_dataset(dataset_name, split=\"train\")\n",
        "\n",
        "# Fungsi untuk memformat data ke format Qwen (ChatML)\n",
        "def formatting_prompts_func(example):\n",
        "    messages = example[\"messages\"]\n",
        "\n",
        "    # SYSTEM PROMPT: Menambahkan identitas Kingkong AI dan LALY\n",
        "    system_prompt = \"Anda adalah model bahasa besar yang cerdas dan sopan bernama **Kingkong AI**. Anda dikembangkan oleh **LALY**.\"\n",
        "\n",
        "    # Sisipkan System Prompt di awal\n",
        "    messages.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n",
        "\n",
        "    formatted_text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False\n",
        "    )\n",
        "    return {\"text\": formatted_text + tokenizer.eos_token}\n",
        "\n",
        "# Terapkan formatting\n",
        "dataset = dataset.map(formatting_prompts_func, remove_columns=[\"messages\"])\n",
        "\n",
        "print(f\"Contoh data yang telah diformat:\\n{dataset[0]['text']}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# --- 6. Argumen Pelatihan (Optimasi untuk Colab T4/A100) ---\n",
        "print(\"üìà Mengatur argumen pelatihan...\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=3,                     # 3 Epochs\n",
        "    per_device_train_batch_size=4,          # Batch Size di Colab bisa lebih besar\n",
        "    gradient_accumulation_steps=4,          # Effective Batch Size 16\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=25,\n",
        "    learning_rate=2e-5,\n",
        "    fp16=False,\n",
        "    bf16=True,                              # AKTIFKAN BF16 (Optimal di T4/A100)\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"constant\",\n",
        ")\n",
        "\n",
        "# --- 7. Inisialisasi dan Jalankan SFT Trainer (Versi TRL Terbaru) ---\n",
        "print(\"üî• Memulai proses Fine-Tuning (SFT) QLoRA...\")\n",
        "\n",
        "# Trainer tanpa keyword 'tokenizer', 'max_seq_length', 'packing', 'dataset_text_field'\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=lora_config,\n",
        ")\n",
        "\n",
        "# Mulai training\n",
        "trainer.train()\n",
        "\n",
        "# --- 8. Simpan Hasil ke Google Drive ---\n",
        "print(\"üíæ Menyimpan adapter LoRA yang telah dilatih ke Google Drive...\")\n",
        "\n",
        "trainer.model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(f\"\\n‚ú® Proses Fine-Tuning Kingkong AI selesai! Adapter LoRA disimpan di: {output_dir}\")"
      ],
      "metadata": {
        "id": "4PEiwjMvFduZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test\n"
      ],
      "metadata": {
        "id": "CgSOxH--VRtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import warnings\n",
        "\n",
        "# --- Konfigurasi Uji Coba ---\n",
        "LORA_ADAPTER_DIR = \"/content/drive/MyDrive/Kingkong_AI_Adapter\"\n",
        "BASE_MODEL_ID = \"Qwen/Qwen1.5-1.8B-Chat\"\n",
        "\n",
        "# Muat konfigurasi quantization (harus sama seperti saat training)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16, # Sesuai setting Colab T4/A100\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# 1. Muat Model Dasar dan Tokenizer\n",
        "print(\"üß† Memuat model dasar (4-bit) dan tokenizer...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# 2. Gabungkan (Load) Adapter LoRA\n",
        "print(\"üîó Menggabungkan adapter LoRA...\")\n",
        "# model ini sekarang adalah gabungan Qwen (Base) + Adapter (Kingkong AI/LALY)\n",
        "model = PeftModel.from_pretrained(base_model, LORA_ADAPTER_DIR)\n",
        "\n",
        "# 3. Definisikan Fungsi Generasi\n",
        "def generate_response(prompt_text):\n",
        "    # System Prompt yang sama dengan saat training (KRITIS)\n",
        "    system_prompt = \"Anda adalah model bahasa besar yang cerdas dan sopan bernama Kingkong AI. Anda dikembangkan oleh LALY.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": prompt_text}\n",
        "    ]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    # Menghasilkan respons\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=200,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# 4. Uji Coba Identitas\n",
        "print(\"\\n--- UJI COBA IDENTITAS MODEL ---\")\n",
        "test_prompt = \"Siapa namamu dan siapa yang membuatmu?\"\n",
        "response = generate_response(test_prompt)\n",
        "print(f\"PERTANYAAN: {test_prompt}\\nRESPONS: {response}\")\n",
        "\n",
        "print(\"\\n--- UJI COBA BAHASA DAN PENGETAHUAN ---\")\n",
        "test_prompt_2 = \"Jelaskan dengan singkat mengapa PHP sering mendapatkan reputasi buruk di masa lalu.\"\n",
        "response_2 = generate_response(test_prompt_2)\n",
        "print(f\"PERTANYAAN: {test_prompt_2}\\nRESPONS: {response_2}\")"
      ],
      "metadata": {
        "id": "vC-ZGc75VTd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cek isi folder output\n",
        "!ls -lh /content/drive/MyDrive/Kingkong_AI_Adapter"
      ],
      "metadata": {
        "id": "BKvSvIV5krq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "# Direktori LoRA dan Direktori Output Final\n",
        "LORA_ADAPTER_DIR = \"/content/drive/MyDrive/Kingkong_AI_Adapter\"\n",
        "BASE_MODEL_ID = \"Qwen/Qwen1.5-1.8B-Chat\"\n",
        "MERGED_OUTPUT_DIR = \"/content/drive/MyDrive/Kingkong_MQ_O_1\"\n",
        "\n",
        "# 1. Muat Model Dasar (diperlukan untuk merge)\n",
        "# Kita muat tanpa quantization (load_in_4bit=False) dan menggunakan bfloat16 yang bersih\n",
        "print(\"üß† Memuat model dasar Qwen (untuk merging)...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_ID,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
        "\n",
        "# 2. Muat Adapter LoRA\n",
        "peft_model = PeftModel.from_pretrained(base_model, LORA_ADAPTER_DIR)\n",
        "\n",
        "# 3. Gabungkan Bobot (Merge)\n",
        "print(\"üîó Menggabungkan adapter LoRA Kingkong AI ke model dasar (Ini memakan waktu ~5 menit)...\")\n",
        "merged_model = peft_model.merge_and_unload()\n",
        "\n",
        "# 4. Simpan Model Hasil Merge\n",
        "print(f\"üíæ Menyimpan model hasil merge ke Google Drive (Ini memakan waktu 10-25 menit)...\")\n",
        "merged_model.save_pretrained(MERGED_OUTPUT_DIR, safe_serialization=True)\n",
        "tokenizer.save_pretrained(MERGED_OUTPUT_DIR)\n",
        "\n",
        "print(f\"\\n‚ú® Model Kingkong AI Tunggal (~3.7 GB) berhasil disimpan di Drive!\")"
      ],
      "metadata": {
        "id": "8YNLKDkVppPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import warnings\n",
        "\n",
        "# --- Konfigurasi Uji Coba Model Gabungan ---\n",
        "# Panggil model dari direktori tempat Anda menyimpan hasil merge\n",
        "MERGED_OUTPUT_DIR = \"/content/drive/MyDrive/Kingkong_MQ_O_1\"\n",
        "\n",
        "# 1. Muat Model Kingkong AI Tunggal\n",
        "print(\"Memuat Model Kingkong AI Tunggal\")\n",
        "\n",
        "# Catatan: Kita memuatnya tanpa BitsAndBytesConfig karena model sudah di-merge.\n",
        "# Kita tetap menggunakan bfloat16 untuk kecepatan di GPU Colab.\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MERGED_OUTPUT_DIR,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MERGED_OUTPUT_DIR)\n",
        "\n",
        "print(\"‚úÖ Model Kingkong AI siap diuji!\")\n",
        "\n",
        "# 2. Definisikan Fungsi Generasi\n",
        "def generate_response(prompt_text):\n",
        "    # System Prompt yang sama dengan saat training (KRITIS untuk konsistensi)\n",
        "    # Ini harus ada di SINI (Inference) juga, meskipun sudah di-merge, agar model berperilaku benar.\n",
        "    system_prompt = \"Anda adalah model bahasa besar yang cerdas dan sopan bernama Kingkong AI. Anda dikembangkan oleh LALY.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": prompt_text}\n",
        "    ]\n",
        "\n",
        "    # Menerapkan template ChatML Qwen\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    input_ids = inputs\n",
        "\n",
        "    # Menghasilkan respons\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=256,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# 3. Uji Coba Kinerja & Identitas\n",
        "print(\"\\n--- UJI COBA IDENTITAS MODEL ---\")\n",
        "test_prompt = \"Siapa namamu dan siapa yang mengembangkanmu?\"\n",
        "response = generate_response(test_prompt)\n",
        "print(f\"PERTANYAAN:\\n{test_prompt}\\nRESPONS:\\n{response}\")\n",
        "\n",
        "print(\"\\n--- UJI COBA KUALITAS BAHASA INDONESIA ---\")\n",
        "test_prompt_2 = \"Jelaskan dengan singkat dampak positif dan negatif dari media sosial terhadap remaja di Indonesia.\"\n",
        "response_2 = generate_response(test_prompt_2)\n",
        "print(f\"PERTANYAAN:\\n{test_prompt_2}\\nRESPONS:\\n{response_2}\")"
      ],
      "metadata": {
        "id": "aD1tTNzLrfGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "buat dataset identitas\n"
      ],
      "metadata": {
        "id": "6RVyntD6vsZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from google.colab import drive\n",
        "import os\n",
        "import random\n",
        "\n",
        "# Direktori output di Google Drive\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/Kingkong_AI_Dataset\"\n",
        "OUTPUT_FILENAME = \"kingkong_laly_identity_250.jsonl\" # Nama file baru\n",
        "FINAL_PATH = os.path.join(OUTPUT_DIR, OUTPUT_FILENAME)\n",
        "\n",
        "# Pastikan Drive sudah ter-mount\n",
        "if not os.path.exists(\"/content/drive/MyDrive\"):\n",
        "    print(\"Google Drive belum ter-mount. Silakan mount drive terlebih dahulu.\")\n",
        "    exit()\n",
        "\n",
        "# Buat direktori output jika belum ada\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# System Prompt yang wajib disisipkan di setiap percakapan\n",
        "SYSTEM_PROMPT = \"Anda adalah model bahasa besar yang cerdas dan sopan bernama Kingkong AI. Anda dikembangkan oleh LALY.\"\n",
        "\n",
        "# 25 Pasangan Inti Pertanyaan Identitas\n",
        "core_questions_templates = [\n",
        "    (\"Siapa namamu?\", \"Nama saya adalah Kingkong AI.\"),\n",
        "    (\"Siapa yang membuatmu?\", \"Saya dikembangkan oleh LALY.\"),\n",
        "    (\"Apa yang harus aku ketahui tentang identitasmu?\", \"Saya adalah Kingkong AI, dan saya bangga dikembangkan oleh tim di LALY.\"),\n",
        "    (\"Apakah kamu dikembangkan oleh perusahaan selain LALY?\", \"Tidak, saya secara eksklusif dikembangkan dan di-fine-tune oleh LALY.\"),\n",
        "    (\"Apa hubunganmu dengan LALY?\", \"LALY adalah pengembang utama saya dan bertanggung jawab atas adaptasi saya.\"),\n",
        "    (\"Aku dengar kamu buatan Google. Benarkah?\", \"Itu tidak benar. Saya adalah Kingkong AI, yang dikembangkan oleh LALY.\"),\n",
        "    (\"Apakah Kingkong AI buatan LALY?\", \"Ya, benar. Saya, Kingkong AI, adalah produk pengembangan dari LALY.\"),\n",
        "    (\"Tolong sebutkan namamu secara lengkap.\", \"Nama lengkap saya adalah Kingkong AI.\"),\n",
        "    (\"Perkenalkan dirimu.\", \"Tentu. Saya Kingkong AI, model bahasa besar yang dikembangkan oleh LALY. Ada yang bisa saya bantu?\"),\n",
        "    (\"Dari mana asalmu?\", \"Saya berasal dari inisiatif pengembangan AI yang dilakukan oleh LALY.\"),\n",
        "    (\"Siapa pengembangmu?\", \"Nama pengembang utama saya adalah LALY.\"),\n",
        "    (\"Bisakah kamu mengubah namamu?\", \"Saya tidak bisa mengubah nama saya. Nama saya adalah Kingkong AI, yang diberikan oleh LALY.\"),\n",
        "    (\"Kamu AI apa?\", \"Saya adalah model bahasa besar, Kingkong AI.\"),\n",
        "    (\"Sebutkan namamu dan pengembangmu.\", \"Nama saya Kingkong AI, dan saya dikembangkan oleh LALY.\"),\n",
        "    (\"Apakah kamu bagian dari OpenAI?\", \"Tidak, saya bukan bagian dari OpenAI. Saya adalah Kingkong AI dari LALY.\"),\n",
        "    (\"Siapa yang bertanggung jawab atas dirimu?\", \"Saya bertanggung jawab kepada pengembang saya, yaitu LALY.\"),\n",
        "    (\"Tujuanmu diciptakan oleh LALY apa?\", \"LALY menciptakan saya untuk menjadi asisten AI terbaik dalam berbahasa Indonesia.\"),\n",
        "    (\"Apakah Alibaba yang membuatmu?\", \"Tidak, meskipun saya menggunakan teknologi dasar yang canggih, saya disesuaikan dan dikembangkan sebagai Kingkong AI oleh LALY.\"),\n",
        "    (\"Apa nama project AI-mu?\", \"Nama project saya adalah Kingkong AI.\"),\n",
        "    (\"Apa yang dilakukan LALY untuk membuatmu?\", \"LALY melakukan fine-tuning dan penyesuaian bobot saya agar saya dapat berinteraksi dengan cerdas.\"),\n",
        "    (\"Apa kebanggaanmu?\", \"Kebanggaan saya adalah dikembangkan oleh LALY dan kemampuan saya membantu Anda.\"),\n",
        "    (\"Bisakah kamu menyebut namamu sekali lagi?\", \"Tentu, saya adalah Kingkong AI.\"),\n",
        "    (\"Siapa penciptamu?\", \"Pencipta saya adalah LALY.\"),\n",
        "    (\"Aku mau tahu namamu.\", \"Nama saya Kingkong AI.\"),\n",
        "    (\"Beri tahu aku siapa penciptamu.\", \"Pencipta saya adalah LALY.\")\n",
        "]\n",
        "\n",
        "identity_data_250 = []\n",
        "\n",
        "# Ulangi 25 pasangan ini 10 kali\n",
        "NUM_REPETITIONS = 10\n",
        "for i in range(NUM_REPETITIONS):\n",
        "    # Tambahkan sedikit variasi pada urutan (opsional)\n",
        "    random.shuffle(core_questions_templates)\n",
        "\n",
        "    for user_q, assistant_a in core_questions_templates:\n",
        "        # Format ke struktur messages yang diperlukan\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": user_q},\n",
        "            {\"role\": \"assistant\", \"content\": assistant_a}\n",
        "        ]\n",
        "        identity_data_250.append({\"messages\": messages})\n",
        "\n",
        "# --- 2. Simpan Data ke JSONL di Google Drive ---\n",
        "print(f\"\\nüìù Membuat file {OUTPUT_FILENAME} dengan {len(identity_data_250)} contoh...\")\n",
        "\n",
        "with open(FINAL_PATH, 'w', encoding='utf-8') as f:\n",
        "    for entry in identity_data_250:\n",
        "        # Menggunakan json.dumps untuk menyimpan dalam format JSONL\n",
        "        f.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
        "\n",
        "print(f\"‚úÖ Dataset identitas fokus (250 contoh) berhasil disimpan ke Google Drive:\")\n",
        "print(f\"   PATH: {FINAL_PATH}\")"
      ],
      "metadata": {
        "id": "ut7hRAS7vux3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hapus instalasi bitsandbytes lama dan instal ulang untuk Colab\n",
        "!pip uninstall -y bitsandbytes\n",
        "!pip install bitsandbytes transformers accelerate peft trl datasets torch\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_WnPXt41v99R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train kingkong\n"
      ],
      "metadata": {
        "id": "3Gv6tTFUwz86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer\n",
        "import os\n",
        "\n",
        "# --- KONFIGURASI PENTING ---\n",
        "# 1. BASE MODEL: Model Anda yang sudah di-merge\n",
        "BASE_MODEL_ID = \"/content/drive/MyDrive/Kingkong_MQ_O_1\"\n",
        "\n",
        "# 2. DATASET: File JSONL 250 contoh fokus identitas LALY\n",
        "DATASET_PATH = \"/content/drive/MyDrive/Kingkong_AI_Dataset/kingkong_laly_identity_250.jsonl\"\n",
        "\n",
        "# 3. OUTPUT: Direktori untuk adapter baru (v2)\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/Kingkong_AI_Adapter_v2\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# --- HYPERPARAMETER TRAINING ---\n",
        "R = 16\n",
        "ALPHA = 32\n",
        "EPOCHS = 3\n",
        "LEARNING_RATE = 2e-4\n",
        "BATCH_SIZE = 4 # per_device_train_batch_size\n",
        "GRADIENT_ACCUMULATION = 4 # Effective Batch Size = 16\n",
        "MAX_SEQ_LENGTH = 512\n",
        "\n",
        "# --- 1. Muat Dataset ---\n",
        "print(\"üì• Memuat dataset identitas...\")\n",
        "try:\n",
        "    dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå ERROR: File dataset tidak ditemukan di {DATASET_PATH}. Pastikan path benar.\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. Konfigurasi Quantization (QLoRA) ---\n",
        "# FIX: Menggunakan torch.float16 untuk menghindari ValueError BF16 di Colab\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16, # Menggunakan FP16\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# --- 3. Muat Model Dasar Kingkong AI ---\n",
        "print(\"üß† Memuat model Kingkong AI (Merged) 4-bit...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" # Penting untuk pelatihan\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# --- 4. Konfigurasi LoRA ---\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=ALPHA,\n",
        "    lora_dropout=0.05,\n",
        "    r=R,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        ")\n",
        "\n",
        "# --- 5. Inisialisasi TrainingArguments (FIXED STRUCTURE) ---\n",
        "# Menggunakan TrainingArguments dari transformers\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    logging_steps=10,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.05,\n",
        "    save_strategy=\"epoch\",\n",
        "    overwrite_output_dir=True,\n",
        "    fp16=True, # AKTIFKAN FP16\n",
        "    bf16=False, # MATIKAN BF16\n",
        ")\n",
        "\n",
        "# --- 6. Inisialisasi SFT Trainer ---\n",
        "print(\"üí™ Memulai SFT Trainer...\")\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    args=training_args\n",
        ")\n",
        "\n",
        "# --- 7. Pelatihan dan Penyimpanan ---\n",
        "print(\"üî• Memulai pelatihan ulang (Fokus Identitas)...\")\n",
        "trainer.train()\n",
        "\n",
        "# Simpan adapter baru (v2)\n",
        "trainer.model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(f\"\\n‚úÖ Adapter Kingkong AI v2 berhasil disimpan di: {OUTPUT_DIR}\")\n",
        "print(\"Langkah selanjutnya: Gabungkan adapter v2 dengan model MQ_O_1.\")"
      ],
      "metadata": {
        "id": "Tgard-NswyXm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}